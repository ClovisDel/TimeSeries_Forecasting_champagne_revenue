---
title: |
  
author: 
- Clovis Deletre
- Charles Vitry
date:
output:
  rmarkdown::html_document:
    theme: cerulean
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float: true
---
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 55px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 38px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 28px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 35px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>


```{r include=FALSE}
library("Metrics")
library(forecast)
library(ggplot2)
library(dplyr)
```

# Introduction

Dans le cadre de ce projet, nous souhaitons modéliser par un processus ARIMA une série temporelle données et réaliser des prévisions dessus.

Ici, la série concerne les ventes de champagne de janvier 1962 à septembre 1970.

L'ambition est de pouvoir effectuer des prévision pour l'année suivante.

## Importation des données et transformation en série temporelle 

Dans un premier temps on import de la base, on ne selectionne que la colonne des valeurs.
```{r}
library(readr)
data <- read_delim("data.csv", 
    delim = ";", locale = locale(encoding = "ISO-8859-1"))
data
```

```{r}
data_value <- data[,2]
summary(data)
```

On transforme la base en série temporelle avec la fonction ts() de R.

On précise la fréquence de la série (ici mensuelle) et la date de début de la série.

On divise la série en deux parties : une partie d'apprentissage et une partie de test à partir des 12 dernières observations
```{r}
data_ts <- ts(data_value, start = 1962, frequency = 12)

data_ts_train <- window(data_ts, start = c(1962, 1), end = c(1969, 9))
data_ts_test <- window(data_ts, start = c(1969, 10), end = c(1970, 9))
```

```{r}
data_ts
```

```{r}
autoplot(data_ts , main = "Série temporelle revenue champagne" , xlab = "Année", ylab = "Valeur")
```


## Détermination des paramètres du modèle 

### Détermination de l'ordre de différentiation (q & Q)

Pour déterminer notre modèle, il faut commencer par avoir une idée série stationnaire.

Pour se faire on décompose la série temporelle en tendance, saisonnalité et bruit

> on remarque rapidement forte saisonnalité (ici annuelle) et une tendance 

```{r}
decompose(data_ts_train) %>% 
  autoplot()
```

Pour commencer on differencie une première fois en saisonnalité donc en application le facteur (I-B^12)

```{r}
data_diff12 <- diff(data_ts_train, 12)

autoplot(data_diff12, main = "Série temporelle revenue champagne (I-B^12)", xlab = "Année", ylab = "Valeur")
```



On décompose la série temporelle différenciée pour l'analyser.

```{r fig.keep='last'}
decompose_diff12 <- decompose(data_diff12)
autoplot(decompose_diff12 , main = "Décomposition additive de la série temporelle revenue champagne (I-B^12)")
```


A l'oeil nu on remarque que la série temporelle n'a plus de tendance, mais on vérifie par le test de Mann-Kendall. 

> test de tendance : Mann-Kendall Test  : https://www.statology.org/mann-kendall-trend-test-r/
(H0) : There is no trend 
(H1) : There is a trend
  
```{r}
library(Kendall)

MannKendall(data_diff12)
```
pvalue = 0.20906 > 0.05 => on ne rejette pas l hypothèse (H0) donc la série n'a pas de tendance

On peut ainsi dire qu'il n'y a pas besoin de différencier en tendance, on estime que (I-B^12) est suffisant et comprend la différentiation en tendance.


### Détermination de p et q (P & Q) 

Maintenant qu on a notre paramètre d et D on cherche les autres paramètres : p et q (P et Q pour la saisonnalité)


Pour trouver la valeur du paramètre q on regarde l ACF, pour trouver à "quel niveau" on doit mettre en place un moyenne mobile.
```{r fig.keep='last'}
acf(data_diff12, lag.max = 24, main = "ACF revenue champagne", ci.type = "ma") %>% 
  autoplot()
```

D'après le corrélograme de (I-B^12)Xt on peut déduire :
> La série est stationnaire
> La série présente des valeurs fortes en ^p(1) et ^p(12), donc probablement introduire un MA
> Dans les premiers niveaux on remarque une valeur en forte en q=1,
On garde un MA(1) pour le moment.


Pour trouver la valeur du paramètre p on regarde la PACF :  
```{r fig.keep='last'}
pacf(data_diff12, main = "PACF revenue champagne") %>% 
  autoplot()
```

Ce qu on peut remarquer, c'est qu'il faut attendre le lag = 12 pour avoir une valeur significative, donc on garde un AR(0) pour le moment.


## Test du modèle retenue :
On obitent donc un SARIMA12 (0,0,1)(0,1,1)

De la forme : (I-B^12)Xt = (1 - θ1)(1 - θ12)εt

On applique notre modèle SARIMA12 (0,0,1)(0,1,1) sur la série temporelle d'apprentissage.
```{r}
model1 <- arima(data_ts_train, order = c(0,0,1), seasonal = list(order = c(0,1,1), period = 12))
model1
```
On obtient les valeurs de nos deux coefs : θ1 = 0.3494 et θ12 = -0.2775.

On obtient également les valeurs de leurs écarts-type : σθ1 = 0.1148 et σθ12 = 0.0977.

AIC à 1344.16, il est utile pour la comparaison de modèles, plus il est petit plus le modèle est bon

On test la significativité de nos coefs :
Pour chaque coef on test :
(H0) : θi = 0
(H1) : θi ≠ 0

```{r}
require(lmtest)
coeftest(model1)
```

Pour les deux θ, le test est hautement signficatif donc on rejette H0. 

## Test des résidus
Maintenant on test les résidus, en effet ils doivent correspondre à un bruit blanc. 

On commence par tester les autocorrélation des résidus εt : 

On test : 
(H0) : les résidus εt sont non corrélés
(H1) : les résidus εt sont corrélés
```{r}
Box.test(model1$residuals, lag = 12, type = "Ljung-Box")
```
On obient une p_value de 0.7071, le test est non significatif, on ne rejette pas H0. 
On peut conclure que les résidus εt sont non corrélés.

On vérifie visuellement nos résultats : 
```{r}
autoplot(model1$residuals, main = "Résidus du modèle SARIMA12 (0,0,1)(0,1,1)", xlab = "Année", ylab = "Valeur")
```
```{r fig.keep='last'}
acf(model1$residuals, lag.max = 24, main = "ACF des résidus du modèle SARIMA12 (0,0,1)(0,1,1)", ci.type = "ma") %>% 
  autoplot()
```
 on a un pic significatif ce qui est normal pour l'acf des résidus.
```{r fig.keep='last'}
pacf(model1$residuals, main = "PACF des résidus du modèle SARIMA12 (0,0,1)(0,1,1)") %>% 
  autoplot()
```


De ce fait on peut conclure que le modèle SARIMA12 (0,0,1)(0,1,1) est bon pour notre série temporelle.


## Prédiction sur la série temporelle de test

```{r}

```

```{r}
pred = predict(model1, 12)
```

```{r}
#y = data.frame(data_ts, c(data_ts_train, predict(model1, 12)))
```

```{r}
forecast(model1, h=12, xreg=fourierf(data_ts,2,0)) %>% 
  autoplot()

```



On vérifie nos résultats aves les mesures suivantes : 

> MAPE : Mean Absolute Percentage Error
> MAE : Mean Absolute Error
> MSE : Mean Squared Error
> RMSE : Root Mean Squared Error

```{r}
MAPE <- mape(data_ts_test, pred$pred)
MAE <- mae(data_ts_test, pred$pred)
MSE <- mse(data_ts_test, pred$pred)
RMSE <- rmse(data_ts_test, pred$pred)

MAPE
MAE
MSE
RMSE

```
```{r}
library(forecast)
fit <- auto.arima(data_ts_train, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)  
fit
```

CONCLURE 