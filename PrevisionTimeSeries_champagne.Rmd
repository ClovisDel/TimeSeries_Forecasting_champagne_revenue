---
title: Projet prévision sur série chronologique par modèle SARIMA
  
author: 
- Clovis Deletre
- Charles Vitry
date:
output:
  rmarkdown::html_document:
    theme: cerulean
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float: true
---

```{=html}
<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 55px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 38px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 28px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 35px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>
```
```{r include=FALSE}
# Librairies
library(readr) # Lecture du document
library("Metrics") # performance modèle
library(forecast) # time series
library(ggplot2) # affichage
library(dplyr) # pipe
library(Kendall) # test de tendance
library(dygraphs) ; library(xts) # affichage prévision intéractif
theme_set(theme_bw())
```

# Introduction

Dans le cadre de ce projet, nous souhaitons **modéliser par un processus** une série temporelle de données.

Ici, la série concerne les ventes de champagne de janvier 1962 à septembre 1970, avec une observation par mois.

L'ambition est de pouvoir effectuer une **prévision** sur les 12 dernières observations grâce à une modélisation sur le reste des données.

## Importation des données

On importe les données, puis on en affiche un résumé.

```{r}
data <- read_delim("data.csv", delim = ";", col_types = cols(date = col_date(format = "%d/%m/%Y")), 
    trim_ws = TRUE)
summary(data)
```

Vérifions l'hypothèse d'**homogéneité de la variance** au cours des années avec le test de bartlett

```{r}
data$date[1:93] %>% 
  format(format = "%Y") %>%
  bartlett.test(data$value[1:93], .)
```

Avec une p-valeur de 0.03, l'hypothèse H0 de l'homogénéité de la variance au cours des années est rejeté.

> Nous aurions pu envisager une transformation de Box-cox.

<br></br>

*Préparons nos données*

1.  On transforme la base en série temporelle avec la fonction ts() de R.

2.  On précise la fréquence de la série (ici mensuelle) et la date de début de la série.

3.  On divise la série en deux parties : une partie d'apprentissage et une partie de test.

```{r}
data_ts <- ts(data[,2], start = 1962, frequency = 12)

data_ts_train <- window(data_ts, start = c(1962, 1), end = c(1969, 9))
data_ts_test <- window(data_ts, start = c(1969, 10), end = c(1970, 9))
```

<br></br>

Affichage de la série

```{r echo=FALSE}
autoplot(data_ts , main = "Série temporelle revenue champagne" , xlab = "Année", ylab = "Valeur")
```

## Détermination des paramètres du modèle

Nous avons 105 observations à notre disposition ce qui est peu, nous rechercherons donc minimiser le nombre de paramètres à estimer afin de s'assurer de la robustesse de ceux-ci.

UN MODELE SARIMA EST DEFINI COMME SUIT

### Détermination de l'ordre de différentiation (d & D)

La série temporelle doit être un processus stationnaire si nous voulons la modéliser par un [sarima.]{.smallcaps}

Pour observer cela, on décompose la série temporelle en tendance, en saisonnalité et en bruit.

```{r}
decompose(data_ts_train) %>% 
  autoplot(main = "Décomposition additive de la série temporelle")
```

> on remarque une **forte saisonnalité** (ici annuelle) et une **tendance**

Pour commencer on **differencie** une première fois en saisonnalité par l'application de l'opérateur $\nabla ^{12} = I - B^{12}$ aux valeurs.

```{r}
data_diff12 <- diff(data_ts_train, 12)

autoplot(data_diff12, main = "Série temporelle revenue champagne (I-B^12)", xlab = "Année", ylab = "Valeur")
```

On décompose la série temporelle différenciée pour l'analyser.

```{r fig.keep='last'}
decompose_diff12 <- decompose(data_diff12)
autoplot(decompose_diff12 , main = "Décomposition additive de la série temporelle revenue champagne (I-B^12)")
```

Après application de l'opérateur $\nabla ^{12}$ on observe que cela n'a pas été suffisant pour éliminer complétement la saisonnalité.

De plus, on observe la disparition de la tendance, cela est dû à la présence du facteur $I - B$ dans la formule de l'opérateur nabla en saisonnalité.

$$ \nabla ^{12} = I - B^{12} = (1 - B)P(B)$$

Vérifions la disparition de la tendance par le test de Mann-Kendall

```{r}


MannKendall(data_diff12)
```

Avec une p-valeur de 0.209, l'hypothèse H0 "*il n'y a aucune tendance*" est accepté. tandis que l'hypothèse alternative H1 "*il y a une tendance*" est rejeté.

Nous avons stationnariser la série et n'appliquerons donc pas un second opérateur nabla sur les données.

Nous avons pu éliminer la tendance, ce qui est positif pour notre modèle. Mais cela induit une augmentation de **l'hétérogénéité de** **la variance** qui est elle négative pour notre modélisation.

Quantifions l'augmentation en effectuant à nouveau un test d'homogénéité de la variance entre les années.

```{r}
df_diff12 <- data.frame(Y=data_diff12, date=round(time(data_diff12)))
bartlett.test(df_diff12$value, df_diff12$date)
```

La dégradation est significative, nous sommes passés d'une p-valeur de 0.03 à 0.001. C'est à dire, d'une probabilité d'homogénéité de la variance entre les années de 3% à 0.1%. {donc là}

### Détermination de p et q (P & Q)

Après estimation de nos paramètres d & D, estimons p et q (P et Q pour la saisonnalité).

Pour trouver la valeur du paramètre **q**, on regarde le **corrélograme** (autocorrélation) de nos données puisque $\forall h > q,$ $  \rho (h) = 0$ , ce qui correspond à $\hat{\rho} (h) \in$ interval de Bartlett. Puisque nous ne pouvons que estimer les \$ \\rho (h)\$

```{r fig.keep='last'}
acf(data_diff12, lag.max = 30, main = "ACF revenue champagne", ci.type = "ma") 
```

D'après le corrélograme de $I - B^{12} Xt$ on remarque que la série présente une valeur significative d'après l'interval de Bartlett en $\hat{\rho} (1)$ et $\hat{\rho} (12)$

-   La série est stationnaire {???}

Cela induirait une modélisation comportant un $MA(12)$ ce qui entrainerait l'estimation de 12 paramètres, ceci est impensable au vu de notre nombre d'observation.

donc probablement introduire un MA \> Dans les premiers niveaux on remarque une valeur en forte en q=1, On garde un MA(1) pour le moment.

Pour trouver la valeur du paramètre p on regarde la PACF :

```{r fig.keep='last'}
pacf(data_diff12, main = "PACF revenue champagne") %>% 
  autoplot()
```

Ce qu on peut remarquer, c'est qu'il faut attendre le lag = 12 pour avoir une valeur significative, donc on garde un AR(0) pour le moment.

## Test du modèle retenue :

On obitent donc un SARIMA12 (0,0,1)(0,1,1)

De la forme : (I-B\^12)Xt = (1 - θ1)(1 - θ12)εt

On applique notre modèle SARIMA12 (0,0,1)(0,1,1) sur la série temporelle d'apprentissage.

```{r}
model1 <- arima(data_ts_train, order = c(0,0,1), seasonal = list(order = c(0,1,1), period = 12))
model1
```

On obtient les valeurs de nos deux coefs : θ1 = 0.3494 et θ12 = -0.2775.

On obtient également les valeurs de leurs écarts-type : σθ1 = 0.1148 et σθ12 = 0.0977.

AIC à 1344.16, il est utile pour la comparaison de modèles, plus il est petit plus le modèle est bon

On test la significativité de nos coefs : Pour chaque coef on test : (H0) : θi = 0 (H1) : θi ≠ 0

```{r}
require(lmtest)
coeftest(model1)
```

Pour les deux θ, le test est hautement signficatif donc on rejette H0.

## Test des résidus

Maintenant on test les résidus, en effet ils doivent correspondre à un bruit blanc.

On commence par tester les autocorrélation des résidus εt :

On test : (H0) : les résidus εt sont non corrélés (H1) : les résidus εt sont corrélés

```{r}
Box.test(model1$residuals, lag = 12, type = "Ljung-Box")
```

On obient une p-valeur de 0.7071, le test est non significatif, on ne rejette pas H0. On peut conclure que les résidus εt sont non corrélés.

On vérifie visuellement nos résultats :

```{r}
autoplot(model1$residuals, main = "Résidus du modèle SARIMA12 (0,0,1)(0,1,1)", xlab = "Année", ylab = "Valeur")
```

```{r fig.keep='last'}
acf(model1$residuals, lag.max = 24, main = "ACF des résidus du modèle SARIMA12 (0,0,1)(0,1,1)", ci.type = "ma") %>% 
  autoplot()
```

on a un pic significatif ce qui est normal pour l'acf des résidus.

```{r fig.keep='last'}
pacf(model1$residuals, main = "PACF des résidus du modèle SARIMA12 (0,0,1)(0,1,1)") %>% 
  autoplot()
```

De ce fait on peut conclure que le modèle SARIMA12 (0,0,1)(0,1,1) est bon pour notre série temporelle.

## Prédiction sur la série temporelle de test

```{r}
pred = predict(model1, 12)
data$prediction_arima = c(rep(NA,93) , pred$pred )
```

Affichage de la prédiction

```{r}


don=xts( x=data[,-1], order.by=data$date)

p <- dygraph(don, main = "Affichage de la présivion par modèle SARIMA",ylab = "Revenue vente de champagne") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.1) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 5, highlightSeriesBackgroundAlpha = 1)  
p
```

On vérifie nos résultats aves les mesures suivantes :

> MAPE : Mean Absolute Percentage Error MAE : Mean Absolute Error MSE : Mean Squared Error RMSE : Root Mean Squared Error

```{r}
MAPE <- mape(data_ts_test, pred$pred)
MAE <- mae(data_ts_test, pred$pred)
MSE <- mse(data_ts_test, pred$pred)
RMSE <- rmse(data_ts_test, pred$pred)

MAPE
MAE
MSE
RMSE

```

CONCLURE
